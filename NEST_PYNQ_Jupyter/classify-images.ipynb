{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSAConnector: libneurosim support not available in NEST.\n",
      "Falling back on PyNN's default CSAConnector.\n",
      "Please re-compile NEST using --with-libneurosim=PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyNN/nest/__init__.py:53: UserWarning: Unable to install NEST extensions. Certain models may not be available.\n",
      "  warnings.warn(\"Unable to install NEST extensions. Certain models may not be available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create C1 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 creation took 67.289864 s\n",
      "Creating S2 layers and reading the epoch weights\n",
      "S2 Shape 11 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2 Shape 8 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2 Shape 6 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2 Shape 4 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting labels took 2.7999999986150215e-05\n",
      "Creating C2 layers\n",
      "C2 creation took 0.559283999999991 s\n",
      "Setting S2 weights to epoch 20\n",
      "Setting C1 spike trains to the training dataset\n",
      ">>>>>>>>> Extracting data samples for fitting <<<<<<<<<\n",
      "========= Start simulation =========\n",
      "Simulating for image number 0\n",
      "Simulating for image number 1\n",
      "Simulating for image number 2\n",
      "Simulating for image number 3\n",
      "Simulating for image number 4\n",
      "Simulating for image number 5\n",
      "Simulating for image number 6\n",
      "Simulating for image number 7\n",
      "Simulating for image number 8\n",
      "Simulating for image number 9\n",
      "Simulating for image number 10\n",
      "Simulating for image number 11\n",
      "Simulating for image number 12\n",
      "Simulating for image number 13\n",
      "Simulating for image number 14\n",
      "Simulating for image number 15\n",
      "Simulating for image number 16\n",
      "Simulating for image number 17\n",
      "Simulating for image number 18\n",
      "Simulating for image number 19\n",
      "========= Stop  simulation =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting C1 spike trains to the validation dataset\n",
      ">>>>>>>>> Extracting data samples for validation <<<<<<<<<\n",
      "========= Start simulation =========\n",
      "Simulating for image number 0\n",
      "Simulating for image number 1\n",
      "Simulating for image number 2\n",
      "Simulating for image number 3\n",
      "Simulating for image number 4\n",
      "Simulating for image number 5\n",
      "Simulating for image number 6\n",
      "Simulating for image number 7\n",
      "Simulating for image number 8\n",
      "Simulating for image number 9\n",
      "Simulating for image number 10\n",
      "Simulating for image number 11\n",
      "Simulating for image number 12\n",
      "Simulating for image number 13\n",
      "Simulating for image number 14\n",
      "Simulating for image number 15\n",
      "Simulating for image number 16\n",
      "Simulating for image number 17\n",
      "Simulating for image number 18\n",
      "Simulating for image number 19\n",
      "========= Stop  simulation =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n",
      "NEST does not allow setting an initial value for isyn_exc\n",
      "NEST does not allow setting an initial value for isyn_inh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVM model onto the training samples\n",
      "Predicting the categories of the validation samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.10      0.18        10\n",
      "           1       0.53      1.00      0.69        10\n",
      "\n",
      "    accuracy                           0.55        20\n",
      "   macro avg       0.76      0.55      0.44        20\n",
      "weighted avg       0.76      0.55      0.44        20\n",
      "\n",
      "[[ 1  9]\n",
      " [ 0 10]]\n"
     ]
    }
   ],
   "source": [
    "#!/bin/ipython\n",
    "\n",
    "# ---LICENSE-BEGIN - DO NOT CHANGE OR MOVE THIS HEADER\n",
    "# This file is part of the Neurorobotics Platform software\n",
    "# Copyright (C) 2014,2015,2016,2017 Human Brain Project\n",
    "#\n",
    "# This program is free software; you can redistribute it and/or\n",
    "# modify it under the terms of the GNU General Public License\n",
    "# as published by the Free Software Foundation; either version 2\n",
    "# of the License, or (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n",
    "# ---LICENSE-END\n",
    "import sys\n",
    "sys.path.append('/home/xilinx/nest/lib/python3.6/site-packages')\n",
    "import numpy as np\n",
    "import pyNN.nest as sim\n",
    "import pathlib as plb\n",
    "import time\n",
    "import pickle\n",
    "import argparse as ap\n",
    "import re\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "import network as nw\n",
    "import visualization as vis\n",
    "\n",
    "# parser = ap.ArgumentParser('./classify-images.py --')\n",
    "# parser.add_argument('--training-c1-dumpfile', type=str, required=True,\n",
    "#                     help='The output file to contain the C1 spiketrains for\\\n",
    "#                          training')\n",
    "# parser.add_argument('--validation-c1-dumpfile', type=str, required=True,\n",
    "#                     help='The output file to contain the C1 spiketrains for\\\n",
    "#                          validation')\n",
    "# parser.add_argument('--training-labels', type=str, required=True,\n",
    "#                     help='Text file which contains the labels of the training\\\n",
    "#                           dataset')\n",
    "# parser.add_argument('--validation-labels', type=str, required=True,\n",
    "#                     help='Text file which contains the labels of the validation\\\n",
    "#                           dataset')\n",
    "# parser.add_argument('--threads', default=1, type=int)\n",
    "# parser.add_argument('--weights-from', type=str, required=True,\n",
    "#                     help='Dumpfile of the S2 weight array')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "training_c1_dumpfile='/home/xilinx/jupyter_notebooks/snn_object_recognition/C1_spikes/train_20imgs_50ms_199px_scales_1.0_0.71_0.5_0.35.bin'\n",
    "validation_c1_dumpfile='/home/xilinx/jupyter_notebooks/snn_object_recognition/C1_spikes/validation_20imgs_50ms_199px_scales_1.0_0.71_0.5_0.35.bin'\n",
    "weights_from='/home/xilinx/jupyter_notebooks/snn_object_recognition/S2_weights/train_20imgs_50ms_199px_scales_1.0_0.71_0.5_0.35_fs3_3prots.bin'\n",
    "training_labels='/home/xilinx/jupyter_notebooks/snn_object_recognition/train.txt'\n",
    "validation_labels='/home/xilinx/jupyter_notebooks/snn_object_recognition/validation.txt'\n",
    "threads=1\n",
    "\n",
    "sim.setup(threads=threads, min_delay=.1)\n",
    "\n",
    "layer_collection = {}\n",
    "\n",
    "# Extracting meta-information about the simulation from the filename\n",
    "training_dumpfile_name = plb.Path(training_c1_dumpfile).stem\n",
    "validation_dumpfile_name = plb.Path(validation_c1_dumpfile).stem\n",
    "training_image_count = int(re.search('\\d*imgs',\n",
    "                                     training_dumpfile_name).group()[:-4])\n",
    "validation_image_count = int(re.search('\\d*imgs',\n",
    "                                       validation_dumpfile_name).group()[:-4])\n",
    "sim_time = float(re.search('\\d*ms', validation_dumpfile_name).group()[:-2])\n",
    "blanktime = 0\n",
    "occurrence = re.search('\\d+\\.\\d+blank', training_dumpfile_name)\n",
    "if occurrence is not None:\n",
    "    blanktime = float(occurrence.group()[:-5])\n",
    "occurrence = re.search('\\d+\\.\\d+blank', validation_dumpfile_name)\n",
    "if occurrence is not None:\n",
    "    blanktime = float(occurrence.group()[:-5])\n",
    "\n",
    "print('Create C1 layers')\n",
    "t1 = time.clock()\n",
    "training_ddict = pickle.load(open(training_c1_dumpfile, 'rb'))\n",
    "validation_ddict = pickle.load(open(validation_c1_dumpfile, 'rb'))\n",
    "layer_collection['C1'] = {}\n",
    "for size, layers_as_dicts in training_ddict.items():\n",
    "    layer_list = []\n",
    "    for layer_as_dict in layers_as_dicts:\n",
    "        n, m = layer_as_dict['shape']\n",
    "        new_layer = nw.Layer(sim.Population(n * m,\n",
    "                        sim.SpikeSourceArray(),\n",
    "                        label=layer_as_dict['label']), (n, m))\n",
    "        layer_list.append(new_layer)\n",
    "    layer_collection['C1'][size] = layer_list\n",
    "print('C1 creation took {} s'.format(time.clock() - t1))\n",
    "\n",
    "print('Creating S2 layers and reading the epoch weights')\n",
    "epoch_weights_list = pickle.load(open(weights_from, 'rb'))\n",
    "epoch = epoch_weights_list[-1][0]\n",
    "weights_dict_list = epoch_weights_list[-1][1]\n",
    "f_s = int(np.sqrt(list(weights_dict_list[0].values())[0].shape[0]))\n",
    "s2_prototype_cells = len(weights_dict_list)\n",
    "layer_collection['S2'] = nw.create_S2_layers(layer_collection['C1'], f_s,\n",
    "                                             s2_prototype_cells, refrac_s2=.1,\n",
    "                                             stdp=False, inhibition=False)\n",
    "\n",
    "print('Creating C2 layers')\n",
    "t1 = time.clock()\n",
    "layer_collection['C2'] = nw.create_C2_layers(layer_collection['S2'],\n",
    "                                             s2_prototype_cells)\n",
    "print('C2 creation took {} s'.format(time.clock() - t1))\n",
    "\n",
    "for pop in layer_collection['C2']:\n",
    "    pop.record('spikes')\n",
    "\n",
    "def set_c1_spiketrains(ddict):\n",
    "    for size, layers_as_dicts in ddict.items():\n",
    "        for layer_as_dict in layers_as_dicts:\n",
    "            spiketrains = layer_as_dict['segment'].spiketrains\n",
    "            dimensionless_sts = [[s for s in st] for st in spiketrains]\n",
    "            the_layer_iter = filter(lambda layer: layer.population.label\\\n",
    "                            == layer_as_dict['label'], layer_collection['C1'][size])\n",
    "            the_layer_iter.__next__().population.set(spike_times=dimensionless_sts)\n",
    "\n",
    "training_labels = open(training_labels, 'r').read().splitlines()\n",
    "validation_labels = open(validation_labels, 'r').read().splitlines()\n",
    "\n",
    "def clear_data(C2_populations):\n",
    "    for pop in C2_populations:\n",
    "        pop.get_data(clear=True)\n",
    "\n",
    "def extract_data_samples(image_count):\n",
    "    samples = []\n",
    "    print('========= Start simulation =========')\n",
    "    for i in range(image_count):\n",
    "        print('Simulating for image number', i)\n",
    "        sim.run(sim_time + blanktime)\n",
    "        spikes =\\\n",
    "            [list(layer_collection['C2'][prot].get_spike_counts().values())[0]\\\n",
    "                for prot in range(s2_prototype_cells)]\n",
    "        samples.append(spikes)\n",
    "        clear_data(layer_collection['C2'])\n",
    "    print('========= Stop  simulation =========')\n",
    "    return samples\n",
    "\n",
    "#### Temporary reduction to every third epoch! #############\n",
    "epoch_weights_list = [index_pair[1] for index_pair in\\\n",
    "                        filter(lambda index_pair: index_pair[0] % 3 == 0,\n",
    "                            zip(range(len(epoch_weights_list)),\n",
    "                                epoch_weights_list))]\n",
    "############################################################\n",
    "\n",
    "for epoch, weights_dict_list in epoch_weights_list:\n",
    "    # Set the S2 weights to those from the file\n",
    "    print('Setting S2 weights to epoch', epoch)\n",
    "    for prototype in range(s2_prototype_cells):\n",
    "        nw.set_s2_weights(layer_collection['S2'], prototype,\n",
    "                          weights_dict_list=weights_dict_list)\n",
    "\n",
    "    training_samples = []\n",
    "    validation_samples = []\n",
    "\n",
    "    print('Setting C1 spike trains to the training dataset')\n",
    "    set_c1_spiketrains(training_ddict)\n",
    "    # Let the simulation run to \"fill\" the layer pipeline with spikes\n",
    "    sim.run(40)\n",
    "    clear_data(layer_collection['C2'])\n",
    "    print('>>>>>>>>> Extracting data samples for fitting <<<<<<<<<')\n",
    "    training_samples = extract_data_samples(training_image_count)\n",
    "    sim.reset()\n",
    "\n",
    "    print('Setting C1 spike trains to the validation dataset')\n",
    "    set_c1_spiketrains(validation_ddict)\n",
    "    # Let the simulation run to \"fill\" the layer pipeline with spikes\n",
    "    sim.run(40)\n",
    "    clear_data(layer_collection['C2'])\n",
    "    print('>>>>>>>>> Extracting data samples for validation <<<<<<<<<')\n",
    "    validation_samples = extract_data_samples(validation_image_count)\n",
    "    sim.reset()\n",
    "\n",
    "    print('Fitting SVM model onto the training samples')\n",
    "\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(training_samples, training_labels)\n",
    "\n",
    "    logfile = open('log_final/{}.log'.format(plb.Path(weights_from).stem), 'a')\n",
    "\n",
    "    print('Predicting the categories of the validation samples')\n",
    "    predicted_labels = clf.predict(validation_samples)\n",
    "    print('============================================================',\n",
    "          file=logfile)\n",
    "    print('Epoch', epoch, file=logfile)\n",
    "    clf_report = metrics.classification_report(validation_labels, predicted_labels)\n",
    "    conf_matrix = metrics.confusion_matrix(validation_labels, predicted_labels)\n",
    "    print(clf_report, file=logfile)\n",
    "    print(clf_report)\n",
    "    print(conf_matrix, file=logfile)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    logfile.close()\n",
    "\n",
    "sim.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
